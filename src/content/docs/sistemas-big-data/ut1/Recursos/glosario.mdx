---
title: 'Glosario de Términos: Big Data'
description: 'Definiciones de los conceptos más importantes de la Unidad de Trabajo 1 sobre Sistemas Big Data.'
---

Este glosario reúne los términos fundamentales que hemos explorado en esta unidad. Te servirá como una guía de referencia rápida para consolidar tu vocabulario en el mundo del Big Data.

---

### A

**Agile Data Science**
:   Una metodología que adapta los principios ágiles (como Scrum) a los proyectos de ciencia de datos, enfocándose en la entrega de valor incremental, la iteración rápida y la colaboración constante.

**Analista de Datos (Data Analyst)**
:   Rol profesional responsable de traducir los datos en información comprensible para el negocio, principalmente a través de informes, dashboards y visualizaciones.

**Apache Hadoop**
:   Un framework de código abierto pionero en el Big Data, diseñado para el almacenamiento y procesamiento distribuido de grandes conjuntos de datos mediante el modelo MapReduce.

**Apache Spark**
:   Un motor de procesamiento de datos unificado y de alta velocidad que puede manejar cargas de trabajo de batch, streaming, machine learning y consultas interactivas. Es conocido por su capacidad de procesamiento en memoria.

**Arquitectura Kappa**
:   Una arquitectura de Big Data que maneja todo el procesamiento de datos (tanto en tiempo real como histórico) a través de una única capa de streaming, simplificando la arquitectura Lambda.

**Arquitectura Lambda**
:   Una arquitectura de Big Data que combina el procesamiento por lotes (capa batch) para obtener vistas precisas y completas de los datos históricos, con el procesamiento en tiempo real (capa speed) para proporcionar vistas rápidas de los datos más recientes.

---

### B

**Big Data**
:   Conjuntos de datos de gran volumen, alta velocidad y/o gran variedad que son demasiado complejos para ser gestionados y procesados con herramientas de software tradicionales. El objetivo es extraer valor de estos datos.

---

### C

**Científico de Datos (Data Scientist)**
:   Rol profesional que utiliza métodos científicos, procesos, algoritmos y sistemas para extraer conocimiento e insights de datos estructurados y no estructurados. Se enfoca en el modelado estadístico y el machine learning.

**Cloud Computing**
:   La entrega de servicios informáticos (incluyendo servidores, almacenamiento, bases de datos, redes, software, análisis e inteligencia) a través de Internet ("la nube") para ofrecer una innovación más rápida y recursos flexibles.

**Complejidad Computacional**
:   El estudio de los recursos (principalmente tiempo y espacio/memoria) que requiere un algoritmo para resolver un problema. Se mide a menudo con la notación asintótica (ej. Big O).

**CRISP-DM (Cross-Industry Standard Process for Data Mining)**
:   Una metodología estándar y cíclica para proyectos de minería de datos y ciencia de datos, que consta de 6 fases: Comprensión del negocio, Comprensión de los datos, Preparación de los datos, Modelado, Evaluación y Despliegue.

---

### D

**Data Lake (Lago de Datos)**
:   Un repositorio centralizado que permite almacenar grandes cantidades de datos brutos en su formato nativo. Es flexible y escalable, ideal para la exploración y el análisis por parte de científicos de datos.

**Data Mesh**
:   Un enfoque de arquitectura de datos descentralizado que organiza los datos por dominios de negocio específicos, tratando los "datos como un producto".

**Data Warehouse (Almacén de Datos)**
:   Un sistema utilizado para el reporting y el análisis de datos. Almacena datos estructurados y preprocesados de una o más fuentes, optimizados para la consulta y la generación de informes.

---

### E

**Edge Computing**
:   Un paradigma de computación distribuida que acerca el procesamiento de datos y el almacenamiento a las fuentes de generación de los mismos (el "borde" de la red). Esto reduce la latencia y el uso de ancho de banda.

**Escalabilidad Horizontal**
:   La capacidad de un sistema para aumentar su rendimiento y capacidad añadiendo más máquinas (nodos) al clúster. Es el enfoque predominante en Big Data.

**Escalabilidad Vertical**
:   La capacidad de un sistema para aumentar su rendimiento y capacidad añadiendo más recursos (CPU, RAM, etc.) a una única máquina existente.

---

### G

**Gobernanza de Datos**
:   El conjunto de procesos, políticas, estándares y métricas que aseguran el uso eficaz y eficiente de la información, permitiendo a una organización alcanzar sus objetivos.

---

### H

**HDFS (Hadoop Distributed File System)**
:   El sistema de archivos distribuidos de Apache Hadoop, diseñado para almacenar archivos muy grandes con acceso de streaming a los datos.

---

### I

**Ingeniero de Datos (Data Engineer)**
:   Rol profesional que diseña, construye y mantiene la infraestructura y las arquitecturas de datos. Son responsables de los pipelines que mueven y transforman los datos.

**Ingesta de Datos**
:   El proceso de importar datos de diversas fuentes a un sistema de almacenamiento (como un Data Lake o Data Warehouse) para su posterior procesamiento y análisis.

---

### K

**KDD (Knowledge Discovery in Databases)**:
:   El proceso de descubrir conocimiento útil en grandes volúmenes de datos. Es una metodología precursora de CRISP-DM.

---

### M

**Machine Learning (ML)**
:   Un subcampo de la inteligencia artificial que se centra en el desarrollo de algoritmos que permiten a los ordenadores aprender de los datos y tomar decisiones o predicciones sin ser explícitamente programados para ello.

**MapReduce**
:   Un modelo de programación y procesamiento utilizado en Apache Hadoop para procesar y generar grandes conjuntos de datos en paralelo en un clúster.

---

### P

**Pipeline de Datos**
:   Una serie de pasos de procesamiento de datos. En un pipeline, la salida de un elemento es la entrada del siguiente. Se utilizan para automatizar el flujo de trabajo de la transformación de datos.

**Procesamiento en Tiempo Real (Streaming)**
:   El análisis de datos de forma continua a medida que se producen o reciben.

**Procesamiento por Lotes (Batch)**
:   El procesamiento de grandes volúmenes de datos de una sola vez, en "lotes" o "tandas". Es adecuado para tareas no urgentes.

---

### S

**SEMMA**
:   Una metodología de minería de datos creada por SAS Institute que consta de 5 fases: Sample (Muestreo), Explore (Exploración), Modify (Modificación), Model (Modelado) y Assess (Evaluación).

---

### V

**7 Vs del Big Data**
:   Un conjunto de características que definen el Big Data: **Volumen** (cantidad), **Velocidad** (rapidez de generación), **Variedad** (tipos de datos), **Veracidad** (calidad y fiabilidad), **Valor** (utilidad), **Variabilidad** (cambios en el significado) y **Visualización** (representación gráfica).