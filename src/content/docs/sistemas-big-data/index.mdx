---
title: "Sistemas de Big Data"
template: splash
draft: true
---

# Propuesta año anterior

## ¿Qué es el Proyecto Integrador?

UT  	    EVALUACIÓN	                HORAS

UT1         Introducción a Big Data	1	10
UT2         Almacenamiento de datos	1	24
UT3         Gestión de Datos	1	4
UT4         Análisis y Búsqueda de Respuestas en Datos	1	4
UT5         Análisis de Datos en R Studio	1 y 2	25
UT6         Bi, Visualización de Datos y cuadros de mando	2	12
UT7         Generación de cuadros de Mando con Power BI	2	21

TOTAL	 	100

## Contenidos mínimos 

Aplicación de técnicas de integración, procesamiento y análisis de información:
–	Conceptos básicos de matemática discreta, lógica algorítmica y complejidad computacional para análisis de datos.
–	Técnicas y procesos de extracción de la información de los datos.
–	Modelado, razonamiento, resolución de problemas.
–	Análisis en tiempo real.
–	Costes y calidad asociados al proceso de análisis de la información.
Configuración de cuadros de mando en entornos computacionales:
–	Técnicas de representación de información. Librerías e implementaciones. Estructuración de datos. Objetivos a cumplir.
–	Cuadro de mando: Fundamentos.
–	Métricas.
–	Principales métodos y algoritmos en la minería de datos. Modelos SEMMA Sample, Explore, Modify, Model, Assess) y CRISP-DM (Cross- Industry Standard Process for Data Mining), entre otros.
–	Fases de los modelos. Valoración. Interpretación. Despliegue.
Gestión y almacenamiento de datos. Búsqueda de respuestas en grandes conjuntos de datos:
–	Sistemas de gestión Almacenamiento.
–	Importación: Flume, Sqoop.
–	Integración de datos.
–	Programación: R y Python.
Aplicación de herramientas para la visualización de datos:
–	Datos no estructurados: Fuentes, tipología.
–	Inteligencia artificial en el análisis de datos.
–	Cluster de máquinas: Información distribuida y redundante.
–	Herramientas de visualización de datos: QlikView, QlikSense, Tableau, Power BI, Domo, Pentaho, MicroStrategy, Business Objects, RJMetrics, Klipfolio, entre otras.
–	Tendencias de visualización de datos.



## RAs 
1. Aplica técnicas de análisis de datos que integran, procesan y analizan la información, adaptando e implementando sistemas que las utilicen.
Criterios de evaluación:
a) Se han identificado conceptos básicos de matemática discreta, lógica algorítmica y complejidad computacional, y su aplicación para el tratamiento automático de la información por medio de sistemas computacionales.
b) Se ha extraído de forma automática información y conocimiento a partir de grandes volúmenes de datos.
c) Se han combinado diferentes fuentes y tipos de datos.
d) Se ha construido un conjunto de datos complejos y se han relacionado entre sí.
e) Se han establecido objetivos y prioridades, secuenciación y organización del tiempo de realización.
f) Se han seleccionado e integrado sistemas de información que satisfacen las necesidades del problema.
g) Se han determinado criterios de coste y calidad necesarios para la eficacia y eficiencia de la implementación de un sistema Big Data.

2. Configura cuadros de mando en diferentes entornos computacionales usando técnicas de análisis de datos.
Criterios de evaluación:
a) Se han clasificado diferentes librerías e implementaciones de las técnicas de representación de la información.
b) Se ha cruzado información sobre el objetivo a conseguir y la naturaleza de los datos.
c) Se ha realizado un cuadro de mandos utilizando técnicas sencillas.
d) Se han utilizado técnicas predictivas complejas para anticiparse a lo que ocurra.
e) Se ha evaluado el impacto del análisis de datos en la consecución de los objetivos propuestos.

3. Gestiona y almacena datos facilitando la búsqueda de respuestas en grandes conjuntos de datos.
Criterios de evaluación:
a) Se han extraído y almacenado datos de diversas fuentes, para ser tratados en distintos escenarios.
b) Se ha fijado el objetivo de extraer valor de los datos para lo que es necesario contar con tecnologías eficientes.
c) Se ha comprobado que la revolución digital exige poder almacenar y procesar ingentes cantidades de datos de distinto tipo y descubrir su valor.
d) Se han desarrollado sistemas de gestión, almacenamiento y procesamiento de grandes volúmenes de datos de manera eficiente y segura, teniendo en cuenta la normativa existente.
e) Se han utilizado habilidades científicas en entornos de trabajo multidisciplinares.

4. Aplica herramientas para la visualización de datos utilizadas en las soluciones Big Data facilitando las tareas de análisis y presentación de resultados.
Criterios de evaluación:
a) Se han examinado distintos escenarios y tipologías de datos no estructurados.
b) Se ha implantado la aplicación de la BI (Business Intelligence) para la extracción de valor.
c) Se ha reconocido la importancia de almacenar grandes volúmenes de datos de forma distribuida y redundante en un clúster de máquinas.
d) Se han determinado las diferencias en el entorno de aplicaciones relacionadas que facilitan el procesamiento de datos de manera rápida, eficiente y eficaz.
e) Se ha comprobado la manera de programar y procesar automáticamente la estructura de datos.
f) Se han valorado las diferentes formas de visualizar los datos que nos interese representar gráficamente, facilitando así las tareas de análisis y presentación de resultados.

# Propuesta propia

## UTs y Contenidos 

- UT1 – Introducción a Big Data (10 h)

Conceptos básicos de Big Data y sus aplicaciones.

Matemática discreta y lógica algorítmica aplicada al análisis de datos.

Complejidad computacional y su relación con el manejo de grandes volúmenes de datos.

Arquitecturas Big Data: Hadoop, Spark, ecosistema.

Modelos y metodologías de análisis de datos: CRISP-DM y SEMMA.

Criterios de calidad, coste y eficiencia en proyectos Big Data.

- UT2 – Almacenamiento de datos (24 h)

Sistemas de bases de datos SQL y NoSQL (MySQL, MongoDB, Cassandra).

Extracción e ingesta de datos con Python (pandas, PySpark, APIs, conectores).

Almacenamiento distribuido: HDFS y almacenamiento en la nube.

Procesamiento en clúster (Spark y PySpark).

Normativa y seguridad en el almacenamiento de datos.

- UT3 – Gestión de Datos (4 h)

Ciclo de vida del dato: recolección, limpieza, transformación, análisis y despliegue.

Estrategias de integración de datos.

Gobierno del dato: calidad, trazabilidad y consistencia.

Organización de proyectos Big Data con Python.

- UT4 – Análisis y Búsqueda de Respuestas en Datos (4 h)

Extracción de información desde datasets complejos.

Preprocesamiento y feature engineering con pandas, NumPy y Scikit-learn.

Introducción al análisis en tiempo real con PySpark Streaming.

Resolución de problemas con algoritmos de búsqueda y optimización.

- UT5 – Análisis de Datos con Python (25 h)

Python para ciencia de datos: pandas, NumPy, Matplotlib, Seaborn.

Análisis exploratorio y descriptivo.

Minería de datos: clustering, clasificación, regresión (Scikit-learn).

Validación y evaluación de modelos.

Preparación de pipelines de datos.

Documentación y automatización de procesos de análisis.

- UT6 – BI, Visualización de Datos y Cuadros de Mando (12 h)

Fundamentos de visualización y BI.

Visualización con Python: Matplotlib, Seaborn, Plotly, Bokeh.

Diseño de dashboards interactivos con Dash o Streamlit.

Selección de métricas y KPIs en proyectos de datos.

Comunicación de resultados de manera clara y efectiva.

- UT7 – Generación de Cuadros de Mando Programados (21 h)

Construcción de aplicaciones interactivas de análisis con Dash/Streamlit.

Integración con APIs y bases de datos.

Automatización de reportes y paneles con Python.

Deploy de dashboards en la web o servidores.

Comparativa de librerías y frameworks para visualización avanzada.